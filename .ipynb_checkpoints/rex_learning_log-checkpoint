Issues:

1) Output from from generator consisted of the input prompt and the answer to the question was cutoff leading to no content.
Prob: generate() returns **input + output**.
Soltuion:
Fix = decode only the new tokens (continuation) and (optionally) wrap with the model’s chat template so it follows instructions better.

2) Still getting cut off, changed the max decode `in config.yaml`
Soltuion:
`max_new_tokens_direct`: 50, it was 16
`max_new_tokens_cot`: 256 # was 128

3) Some of the Cot goes straight to the answer Yes, no, etc. without giving reasoning
Reason: Even with max_new_cot = 256, the model won’t necessarily produce long reasoning unless the prompt forces separation.They need to be told very explicitly that they must produce two sections.
Soltuion: change Cot prompt to be more specific.

No reasoning → The model ignores the "Reasoning:" instruction because the prompt isn’t strong enough. Models often shortcut straight to the final answer.
Raw model cut off → That’s expected when text is truncated by max_new_tokens. If it’s stopping before 256, it’s likely because it thinks the task is done, not because of truncation.
Model answer not restricted → We’re just splitting out "Final Answer:", but the model sometimes puts numbers, citations, or entire sentences there. That leaks into your model_answer column instead of being yes/no/maybe/not in context.
Soltuion: change Cot prompt

